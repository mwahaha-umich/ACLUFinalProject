{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACL U Project Part1b Combine Data Spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjRG9q12NM0"
      },
      "source": [
        "The purpose of this Jupyter notebook is to combine and split the data from the various source files we were provided. The current format is extremely redundant, will cause memory issues if handled directly in its entirety. By combining and splitting the data we will have a much easier time working with the dataset.\n",
        "\n",
        "This notebook splits the Combined input into the following files:\n",
        "\n",
        "\n",
        "*   dfMerged.csv - Includes all voting information (need to rename)\n",
        "*   dfVoters.csv - Includes information with regard to the voters\n",
        "*   dfOutreach.csv - Includes information based on which outreach effort(s) affected a particular voter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PGCBCPTmylJ"
      },
      "source": [
        "# Establish Google Drive Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsdbKMXLtahY",
        "outputId": "bbc8503e-a843-4cf8-a524-36b947d419be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8apoSTF518Wa"
      },
      "source": [
        "## Inserting paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAp6S8A4tgjs"
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  #We keep changing the different paths, so I'm going to check if various configurations exist.\n",
        "  if (os.path.isdir(\"/content/drive/MyDrive/Shared with me/content/drive/My Drive/Shared with me/ACLU/\")):\n",
        "    GoogleDriveBase = \"/content/drive/MyDrive/Shared with me/content/drive/My Drive/Shared with me/\" #Anupriya\n",
        "    WorkingDirectory = GoogleDriveBase + 'ACLU/' #Anupriya\n",
        "  elif (os.path.isdir(\"/content/drive/MyDrive/Projects/ACLU\")):\n",
        "    GoogleDriveBase = \"/content/drive/MyDrive/\" #Mackenzie\n",
        "    WorkingDirectory = GoogleDriveBase + \"Projects/ACLU/\" #Mackenzie\n",
        "  else:  \n",
        "    if (os.path.isdir(\"/content/drive/My Drive/Projects/ACLU/\")):\n",
        "      GoogleDriveBase = \"/content/drive/My Drive/\" #Kyle\n",
        "      WorkingDirectory = GoogleDriveBase + \"Projects/ACLU/\" #Kyle\n",
        "else: # We're not running in Google Colab, which means we're probably running locally. \n",
        "  #Put code here for local copies of the files\n",
        "  GoogleDriveBase = \"\" \n",
        "  WorkingDirectory = GoogleDriveBase + \"\" \n",
        "\n",
        "\n",
        "WorkingFiles = WorkingDirectory + 'WorkingFiles/'\n",
        "BasePickeDrive = GoogleDriveBase + WorkingDirectory + \"Pickle/\"\n",
        "\n",
        "\n",
        "#Make the necessary folders for the script to run.\n",
        "ListOfAllRequiredDirectories = [WorkingDirectory + 'Pickle', \n",
        "                          WorkingDirectory + 'WorkingFiles',\n",
        "                          WorkingDirectory + 'AdditionalData',\n",
        "                          WorkingDirectory + 'ACLUData']\n",
        "\n",
        "for folder in ListOfAllRequiredDirectories:\n",
        "  RunningPath = GoogleDriveBase + folder + \"/\"\n",
        "  Path(RunningPath).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0tJz3qkm6UN"
      },
      "source": [
        "# Prep Items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCNEYtHltidD"
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import csv\n",
        "import string\n",
        "import pandas as pd\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "import shutil\n",
        "import os\n",
        "import unittest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6HDYcJ3YTAx"
      },
      "source": [
        "Now, to ensure that there is nothing left over from previous runs, we're going to delete all the files in our \"WorkingFiles\" directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pn0GhfNroQJ"
      },
      "source": [
        "def CleanWorkspace():\n",
        "  dir = WorkingFiles\n",
        "  for files in os.listdir(dir):\n",
        "      path = os.path.join(dir, files)\n",
        "      try:\n",
        "          shutil.rmtree(path)\n",
        "      except OSError:\n",
        "          os.remove(path)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2vNaMs6nI0T"
      },
      "source": [
        "# Install Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh5NCoc8fsSO",
        "outputId": "193d87a3-d27d-4bcd-dca9-4510784cd70a"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-2.4.1-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLA40XI6weBb",
        "outputId": "5a34d42f-b549-45f2-f3cd-50eb04d5c848"
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/\n",
        "import os\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "if os.path.exists(\"spark-3.1.2-bin-hadoop3.2.tgz\") == False:\n",
        "  !wget -q https://apache.claz.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz #This has failed a few times, you may need to use a mirror from another site: https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar -xvzf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-3.1.2-bin-hadoop3.2/\n",
            "spark-3.1.2-bin-hadoop3.2/R/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/sparkr.zip\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/worker/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/worker/worker.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/profile/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/profile/shell.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/profile/general.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/doc/index.html\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/NAMESPACE\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/html/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/html/R.css\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/html/00Index.html\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/INDEX\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/AnIndex\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.1.2-bin-hadoop3.2/R/lib/SparkR/help/paths.rds\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/workers.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-workers.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-worker.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-thriftserver.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-slaves.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-slave.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-master.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-history-server.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/stop-all.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-workers.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-worker.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-thriftserver.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-slaves.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-slave.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-master.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-history-server.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/start-all.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/spark-daemons.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/spark-daemon.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/slaves.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/decommission-worker.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/decommission-slave.sh\n",
            "spark-3.1.2-bin-hadoop3.2/sbin/spark-config.sh\n",
            "spark-3.1.2-bin-hadoop3.2/python/\n",
            "spark-3.1.2-bin-hadoop3.2/python/dist/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/requires.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/python/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/python/pyspark/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__pycache__/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__pycache__/install.cpython-38.pyc\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/evaluation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/common.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/common.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/clustering.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/classification.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/util.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tree.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tree.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/clustering.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/classification.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/test.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/test.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/stat/_statistics.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/regression.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/recommendation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/recommendation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/random.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/random.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/fpm.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/fpm.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/feature.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/mllib/evaluation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/wrapper.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/wrapper.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/util.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tree.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/stat.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/stat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/regression.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/recommendation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/pipeline.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/pipeline.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/shared.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/shared.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/param/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/linalg/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/image.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/image.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/fpm.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/feature.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/evaluation.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/common.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/common.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/clustering.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/classification.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/base.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tuning.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tuning.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/tree.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/recommendation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/fpm.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/feature.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/evaluation.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/clustering.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/classification.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/ml/base.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/join.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/java_gateway.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/install.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/find_spark_home.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/files.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/files.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/daemon.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/context.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/conf.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/broadcast.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/broadcast.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/accumulators.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/_globals.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/window.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/window.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/types.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/types.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/streaming.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/types.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/group.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/group.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/context.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/conf.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/avro/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/_typing.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/__init__.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/streaming.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/functions.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/column.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/catalog.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/shuffle.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/shell.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resultiterable.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resultiterable.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/requests.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/requests.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/information.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/information.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/profile.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/resource/profile.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/rddsampler.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/py.typed\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/profiler.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/profiler.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/worker.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/version.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/taskcontext.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/version.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/traceback_utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_worker.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_serializers.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_rdd.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_profiler.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_join.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_daemon.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/streamingutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/sqlutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/mlutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/mllibutils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/testing/utils.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/taskcontext.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/util.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/listener.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/listener.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/kinesis.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/kinesis.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/dstream.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/context.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/context.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/__init__.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/streaming/dstream.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/storagelevel.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/storagelevel.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/status.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/status.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/statcounter.pyi\n",
            "spark-3.1.2-bin-hadoop3.2/python/pyspark/statcounter.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/pylintrc\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.ss.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.sql.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/migration_guide/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/quickstart.ipynb\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/getting_started/install.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/testing.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/setting_ide.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/debugging.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/development/contributing.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/autosummary/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/css/\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/_static/copybutton.js\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/index.rst\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/source/conf.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/make.bat\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/make2.bat\n",
            "spark-3.1.2-bin-hadoop3.2/python/docs/Makefile\n",
            "spark-3.1.2-bin-hadoop3.2/python/README.md\n",
            "spark-3.1.2-bin-hadoop3.2/python/MANIFEST.in\n",
            "spark-3.1.2-bin-hadoop3.2/python/.gitignore\n",
            "spark-3.1.2-bin-hadoop3.2/python/.coveragerc\n",
            "spark-3.1.2-bin-hadoop3.2/python/setup.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/run-tests.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/run-tests-with-coverage\n",
            "spark-3.1.2-bin-hadoop3.2/python/mypy.ini\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/userlibrary.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/userlib-0.1.zip\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/text-test.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people_array.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people1.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/people.json\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/sql/ages.csv\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/sub_hello/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/hello/hello.txt\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/sitecustomize.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/coverage_daemon.py\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/conf/\n",
            "spark-3.1.2-bin-hadoop3.2/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.1.2-bin-hadoop3.2/python/setup.cfg\n",
            "spark-3.1.2-bin-hadoop3.2/python/run-tests\n",
            "spark-3.1.2-bin-hadoop3.2/bin/\n",
            "spark-3.1.2-bin-hadoop3.2/bin/sparkR2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/sparkR.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/sparkR\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-submit2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-submit.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-submit\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-sql2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-sql.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-sql\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-shell2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-shell.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-shell\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-class2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-class.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/spark-class\n",
            "spark-3.1.2-bin-hadoop3.2/bin/run-example.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/run-example\n",
            "spark-3.1.2-bin-hadoop3.2/bin/pyspark.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/load-spark-env.sh\n",
            "spark-3.1.2-bin-hadoop3.2/bin/load-spark-env.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/find-spark-home.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/find-spark-home\n",
            "spark-3.1.2-bin-hadoop3.2/bin/docker-image-tool.sh\n",
            "spark-3.1.2-bin-hadoop3.2/bin/beeline.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/beeline\n",
            "spark-3.1.2-bin-hadoop3.2/bin/pyspark2.cmd\n",
            "spark-3.1.2-bin-hadoop3.2/bin/pyspark\n",
            "spark-3.1.2-bin-hadoop3.2/README.md\n",
            "spark-3.1.2-bin-hadoop3.2/conf/\n",
            "spark-3.1.2-bin-hadoop3.2/conf/workers.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/spark-env.sh.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/spark-defaults.conf.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/metrics.properties.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/log4j.properties.template\n",
            "spark-3.1.2-bin-hadoop3.2/conf/fairscheduler.xml.template\n",
            "spark-3.1.2-bin-hadoop3.2/data/\n",
            "spark-3.1.2-bin-hadoop3.2/data/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/data/streaming/AFINN-111.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_svm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_movielens_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_lda_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/ridge-data/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/pic_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/pagerank_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/kmeans_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/iris_libsvm.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/license.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/images/license.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/gmm_data.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/als/\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/als/test.data\n",
            "spark-3.1.2-bin-hadoop3.2/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/graphx/\n",
            "spark-3.1.2-bin-hadoop3.2/data/graphx/users.txt\n",
            "spark-3.1.2-bin-hadoop3.2/data/graphx/followers.txt\n",
            "spark-3.1.2-bin-hadoop3.2/NOTICE\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-zstd.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-spire.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-slf4j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-scopt.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-scala.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-respond.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-re2j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-py4j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-protobuf.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-paranamer.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-netlib.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-mustache.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-modernizr.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-minlog.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-machinist.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-kryo.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jquery.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-join.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jodd.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jline.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-javolution.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-javassist.html\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-janino.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-f2j.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-datatables.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-automaton.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-arpack.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-antlr.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-CC0.txt\n",
            "spark-3.1.2-bin-hadoop3.2/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.1.2-bin-hadoop3.2/LICENSE\n",
            "spark-3.1.2-bin-hadoop3.2/examples/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/users.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/users.orc\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/users.avro\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/user.avsc\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.txt\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.json\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.csv\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/kv1.txt\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/full_user.avsc\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/employees.json\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/survreg.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/mlp.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/ml.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/logit.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/lda.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/kstest.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/glm.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/gbt.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/fpm.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/ml/als.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/dataframe.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/data-manipulation.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/transitive_closure.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/status_api_demo.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/hive.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/basic.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/arrow.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sql/datasource.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/sort.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/pi.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/pagerank.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/als_example.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/logistic_regression.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/kmeans.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/python/als.py\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scripts/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.1.2-bin-hadoop3.2/examples/jars/\n",
            "spark-3.1.2-bin-hadoop3.2/examples/jars/spark-examples_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/python_executable_check.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/pyfiles.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/py_container_checks.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/decommissioning.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/tests/autoscale.py\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.1.2-bin-hadoop3.2/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.1.2-bin-hadoop3.2/yarn/\n",
            "spark-3.1.2-bin-hadoop3.2/yarn/spark-3.1.2-yarn-shuffle.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/\n",
            "spark-3.1.2-bin-hadoop3.2/jars/zstd-jni-1.4.8-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/zookeeper-3.4.14.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/xz-1.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/woodstox-core-5.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/velocity-1.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/transaction-api-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/token-provider-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/super-csv-2.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/stream-2.9.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/stax2-api-3.1.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/stax-api-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-yarn_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-tags_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-tags_2.12-3.1.2-tests.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-streaming_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-sql_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-sketch_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-repl_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-network-shuffle_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-network-common_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-mllib_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-mllib-local_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-mesos_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-launcher_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-kvstore_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-kubernetes_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-hive_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-hive-thriftserver_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-graphx_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-core_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/spark-catalyst_2.12-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/snappy-java-1.1.8.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/snakeyaml-1.24.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/shims-0.9.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-library-2.12.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/re2j-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/pyrolite-4.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/py4j-0.10.9.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-format-2.4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-common-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/parquet-column-1.10.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/paranamer-2.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/oro-2.0.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/orc-shims-1.5.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/orc-mapreduce-1.5.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/orc-core-1.5.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/opencsv-2.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/okio-1.14.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/okhttp-3.12.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/okhttp-2.7.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/objenesis-2.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/nimbus-jose-jwt-4.41.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/netty-all-4.1.51.Final.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/minlog-1.3.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-json-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/metrics-core-4.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/lz4-java-1.7.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/log4j-1.2.17.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/libthrift-0.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/libfb303-0.9.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-storageclass-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-settings-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-scheduling-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-rbac-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-policy-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-networking-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-metrics-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-extensions-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-events-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-discovery-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-core-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-coordination-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-common-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-certificates-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-batch-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-autoscaling-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-apps-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-apiextensions-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-model-admissionregistration-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kubernetes-client-4.12.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-xdr-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-util-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-pkix-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-config-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerby-asn1-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-util-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-simplekdc-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-server-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-identity-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-crypto-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-core-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-common-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-client-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/kerb-admin-1.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jta-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jsr305-3.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jsp-api-2.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-scalap_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-jackson_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-core_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json4s-ast_2.12-3.7.0-M5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json-smart-2.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/json-1.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jpam-1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jodd-core-3.5.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/joda-time-2.10.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jline-2.14.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-server-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-media-jaxb-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-hk2-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-container-servlet-core-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-container-servlet-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-common-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jersey-client-2.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jdo-api-3.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jcip-annotations-1.0-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jaxb-api-2.2.11.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javolution-5.5.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javax.inject-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/janino-3.0.16.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-jaxrs-json-provider-2.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-jaxrs-base-2.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-datatype-jsr310-2.11.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-core-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/httpcore-4.4.12.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/httpclient-4.5.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/htrace-core4-4.1.0-incubating.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hk2-api-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-vector-code-gen-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-storage-api-2.7.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-scheduler-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-common-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-shims-0.23-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-service-rpc-3.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-serde-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-metastore-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-llap-common-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-jdbc-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-exec-2.3.7-core.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-common-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-cli-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hive-beeline-2.3.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-server-web-proxy-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-server-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-registry-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-client-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-yarn-api-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-jobclient-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-core-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-mapreduce-client-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-hdfs-client-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-common-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-client-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-auth-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/hadoop-annotations-3.2.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/guice-servlet-4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/guice-4.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/guava-14.0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/gson-2.2.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/generex-1.0.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/ehcache-3.3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/dnsjava-2.1.7.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/derby-10.12.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/curator-framework-2.13.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/curator-client-2.13.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/core-1.1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-text-1.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-pool-1.5.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-net-3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-math3-3.4.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-logging-1.1.3.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-lang3-3.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-lang-2.6.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-io-2.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-httpclient-3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-dbcp-1.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-daemon-1.0.13.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-configuration2-2.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-compress-1.20.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-collections-3.2.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-codec-1.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-cli-1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/chill_2.12-0.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/chill-java-0.9.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/breeze_2.12-1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/avro-1.8.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/automaton-1.11-8.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-vector-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-memory-netty-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-memory-core-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arrow-format-2.0.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/aopalliance-1.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/antlr4-runtime-4.8-1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/aircompressor-0.10.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/activation-1.1.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/accessors-smart-1.2.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/ST4-4.0.4.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/RoaringBitmap-0.9.0.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/JTransforms-3.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/JLargeArrays-1.5.jar\n",
            "spark-3.1.2-bin-hadoop3.2/jars/HikariCP-2.5.1.jar\n",
            "spark-3.1.2-bin-hadoop3.2/RELEASE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "DZptkUu79PbY",
        "outputId": "ef624a33-c18a-462e-da76-7b3a220ef7b9"
      },
      "source": [
        "import findspark\n",
        "\n",
        "!ls\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] pyspark-shell\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .config(\"spark.driver.memory\", \"12g\") \\\n",
        "        .config(\"spark.driver.maxResultSize\", \"5g\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  spark-3.1.2-bin-hadoop3.2  spark-3.1.2-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://729676b50e51:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff82a065390>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WvfNVKw4U1p"
      },
      "source": [
        "#If you want to see the spark UI\n",
        "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#!unzip ngrok-stable-linux-amd64.zip\n",
        "#get_ipython().system_raw('./ngrok http 4050 &')\n",
        "#!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4T4KNgr3eG"
      },
      "source": [
        "# Combine the Voter Data from the ACLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xPOq2ihL6KO"
      },
      "source": [
        "AllFilesInTheSubfolder = []\n",
        "for subfolder in os.walk(WorkingDirectory + \"ACLUData\"):\n",
        "  #Skip the pickle folder\n",
        "  if \"Pickle\" in subfolder[0] or \"WorkingFiles\" in subfolder[0]:\n",
        "      continue\n",
        "  for file in subfolder[2]:\n",
        "    FileWithPath = subfolder[0] +\"/\"+ file\n",
        "    match = re.search(\"(.*)(text|mail|phone|call|postcards)\", file.lower())\n",
        "    if match == None:\n",
        "      raise ValueError(\"Could not parse: \" + file + \"\\n Please take a look at the type of communication and ensure that it's on the list. \\nThis should only happen when we add new files whose name is inconsistant with the older files.\")\n",
        "    CommunicationType = match.group(2).replace(\"_\", \" \").strip()\n",
        "    #We have some inconsistant communication type names. So, I'm doing a little cleanup here.\n",
        "    if CommunicationType == \"call\":\n",
        "      CommunicationType = \"phone\"\n",
        "    if CommunicationType == \"postcards\":\n",
        "      CommunicationType = \"mail\"\n",
        "    FileWithDetails = (FileWithPath, match.group(1).replace(\"_\", \" \").strip(), match.group(2).replace(\"_\", \" \").strip(), file)\n",
        "    AllFilesInTheSubfolder.append(FileWithDetails)\n",
        "\n",
        "    #print(FileWithDetails)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7A1aHu6iYuj",
        "outputId": "25e7c0c2-fedd-4dbc-9b85-4873e7446f2f"
      },
      "source": [
        "if (len(AllFilesInTheSubfolder) < 25): #Set to more than 25 in case we receive more files.\n",
        "  raise Exception(\"We should have at least 25 data files. Please double check you have loaded all the files.\") \n",
        "else:\n",
        "  print(\"Passed Sanity Check, at least 25 data files.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Passed Sanity Check, at least 25 data files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7HbTjsOA8HF"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "columns = []\n",
        "\n",
        "firstDataFrame = None\n",
        "\n",
        "for file in AllFilesInTheSubfolder:\n",
        "  df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").load(file[0])\n",
        "  df = df.withColumn(\"ElectionType\", F.lit(file[1]))\n",
        "  df = df.withColumn(\"CommunicationType\", F.lit(file[2]))\n",
        "  df = df.withColumn(\"File\", F.lit(file[3]))\n",
        "  columns.extend(df.columns)\n",
        "  if firstDataFrame is None:\n",
        "    firstDataFrame = df\n",
        "  else:\n",
        "    FirstNotSelected = set(firstDataFrame.columns) - set(df.columns)\n",
        "    for value in FirstNotSelected:\n",
        "      df = df.withColumn(value, F.lit(\"\"))\n",
        "    SecondNotSelected = set(df.columns) - set(firstDataFrame.columns)\n",
        "    for value in SecondNotSelected:\n",
        "      firstDataFrame = firstDataFrame.withColumn(value, F.lit(\"\"))\n",
        "    firstDataFrame = firstDataFrame.union(df.select(firstDataFrame.columns))\n",
        "  #Count takes a bit\n",
        "  #print(firstDataFrame.count())\n",
        "columns = set(columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3p4wFkma3l7",
        "outputId": "60d1c1d4-1702-4e46-dbf3-4476017d26a0"
      },
      "source": [
        "print(firstDataFrame.count())\n",
        "len(AllFilesInTheSubfolder)\n",
        "if (firstDataFrame.count() < 3000000):\n",
        "  raise Exception(\"We should have at least 3 Million records after the join proccess. Please double check your data.\")\n",
        "else:\n",
        "  print(\"Passed Sanity Check: Number of records greater than 3 Million.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3196831\n",
            "Passed Sanity Check: Number of records greater than 3 Million.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiCnwHgQz88x"
      },
      "source": [
        "In the following section we will be pulling out all unique voter information.\n",
        "\n",
        "Please Note: As this data includes real world PII we will be making an effort to never show the data from this dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ3cj_2PN3Kc",
        "outputId": "971c3316-2a83-4140-cfb1-b8b3bf252838"
      },
      "source": [
        "#It looks like \"Voter File VANID\" is a unique key for each user, so I'm going to pull out any other person information and stick that into another dataframe. This will reduce memory footprint, which will be necessary for the pivot\n",
        "#that's coming up.\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql import DataFrameWriter\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "dataframesbytypejoinedwithoutidvalues = {}\n",
        "\n",
        "id_columns = [\"Voter File VANID\", \"FirstName\", \"MiddleName\", \"LastName\", \"Address\", \"DWID\",  'Age', 'Primary19', 'MaritalStatus', 'mCity', 'Mass_Incarceration', 'StreetType', 'StreetSuffix',\n",
        "  'mAddress', 'mState', 'mZip5', 'mZip4', 'Sex', 'Suffix', 'CD', 'SD', 'HD', 'PreferredEmail', 'Preferred Phone', 'Cell Phone', 'CountyName', 'DOB', 'DateReg', 'Home Phone', \n",
        "  'EthnicCatalistName', 'Party', 'PersonalEmail', 'RaceName', 'StreetNo', 'StreetNoHalf', 'StreetPrefix', 'StreetName', 'AptType', 'AptNo', \n",
        "  '2020_Biden_Support', 'Voting_Aug_Prim', 'PoliceAccountability', 'VBM_Application', 'MarijuanaConviction',  'Absentee_Voting'] #, 'VotedStatus', 'Voting_Nov_Gen','Zip5', 'City', 'State', 'Zip4', 'City.1', 'State.1', 'Zip5.1', 'Zip4.1', \n",
        "primary_key = [\"Voter File VANID\"]\n",
        "dfVoters = None\n",
        "\n",
        "dfVoters = firstDataFrame.select(id_columns)\n",
        "dfVoters = dfVoters.na.fill(0)\n",
        "\n",
        "WindowSpec  = Window.partitionBy(\"Voter File VANID\").orderBy(\"Age\")\n",
        "dfVoters = dfVoters.withColumn(\"row_number\",row_number().over(WindowSpec))\n",
        "dfVoters = dfVoters.where(dfVoters['row_number'] == 1)\n",
        "\n",
        "print(dfVoters.count())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1633349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2GbhPkVdTeL"
      },
      "source": [
        "Now we will make functions to save the dataframes we create: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhvMQ0l1pQId"
      },
      "source": [
        "\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def SaveSingleFile(df, fileName):\n",
        "  tempDir = WorkingFiles + \"temp\"\n",
        "  if not os.path.exists(tempDir):\n",
        "    os.makedirs(tempDir)\n",
        "  else:\n",
        "    shutil.rmtree(tempDir)\n",
        "    os.makedirs(tempDir)\n",
        "  print(\"Writing File\")\n",
        "  df.coalesce(1).write.csv(WorkingFiles + \"temp\", mode=\"overwrite\", header=True)\n",
        "  #df.write.csv(WorkingFiles + \"temp\", mode=\"overwrite\", header=True)\n",
        "  os.chdir(WorkingFiles + \"temp\")\n",
        "  print(\"Moving file\")\n",
        "  for file in glob.glob('part-00000-*'):\n",
        "      shutil.move(file, fileName)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i6RSIPlmAky",
        "outputId": "5d4d8c62-b8c3-4e81-ca62-b17cb2ad9bca"
      },
      "source": [
        "\n",
        "SaveSingleFile(dfVoters, WorkingFiles + \"dfVotersSingle.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28cd4erdedu"
      },
      "source": [
        "Creating our dfMerged file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxcryyg6w3Ka"
      },
      "source": [
        "ElectionColumns = [\"Voter File VANID\", \"ElectionType\", \"CommunicationType\", \"File\"]\n",
        "\n",
        "RemainingColumns = list(set(firstDataFrame.columns) - (set(id_columns + ElectionColumns)))\n",
        "dfMerged = firstDataFrame.select(RemainingColumns)\n",
        "\n",
        "#print(dfMerged.show())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBYXQvfTHwBm",
        "outputId": "8069156b-fa12-4fab-a4ab-9e52994b8129"
      },
      "source": [
        "SaveSingleFile(dfMerged, WorkingFiles + \"dfMerged.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x96V5DANdofX"
      },
      "source": [
        "Adding our Age Group buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD9taegpk5mr",
        "outputId": "a843b1ae-124e-428d-e7fa-243b7c11f2ea"
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "Bucketized = spark.read.option(\"header\",True).csv(WorkingFiles + \"dfMerged.csv\")\n",
        "\n",
        "#dfVoters_sub = firstDataFrame.select(['Voter File VANID', 'Age', \n",
        "       #'MaritalStatus', 'Sex', 'CountyName',\n",
        "       #'Party', 'RaceName', 'Primary19', '2020_Biden_Support', \n",
        "       #'PoliceAccountability','VBM_Application', 'MarijuanaConviction','Absentee_Voting', 'Mass_Incarceration'])\n",
        "\n",
        "dfVoters_sub = firstDataFrame\n",
        "\n",
        "bins= [18.0,29.0,44.0,64.0, 200.0]\n",
        "labels = ['18-29','30-44','45-64','65+']\n",
        "print(dfVoters_sub.head)\n",
        "dfVoters_sub = dfVoters_sub.withColumn(\"AgeInt\", firstDataFrame[\"Age\"].cast(IntegerType()))\n",
        "Bucketized = Bucketizer().setInputCol(\"AgeInt\").setOutputCol( \"age_bucket\").setSplits( bins ).transform(dfVoters_sub)#.collect()\n",
        "\n",
        "t = {'0.0':'18-29', '1.0': '30-44', '2.0':'45-64', '3.0': '65+'}\n",
        "\n",
        "Bucketized = Bucketized.withColumn(\"AgeGroup\", Bucketized[\"age_bucket\"].cast(\"string\"))\n",
        "Bucketized = Bucketized.replace(t, \"AgeGroup\")\n",
        "\n",
        "\n",
        "\n",
        "SaveSingleFile(Bucketized, WorkingFiles + \"Bucketized.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method DataFrame.head of DataFrame[Voter File VANID: string, DWID: string, mAddress: string, mCity: string, mState: string, mZip5: string, mZip4: string, Sex: string, Address: string, City9: string, State10: string, Zip511: string, Zip412: string, LastName: string, FirstName: string, MiddleName: string, Suffix: string, CD: string, SD: string, HD: string, PreferredEmail: string, Preferred Phone: string, Age: string, Cell Phone: string, CountyName: string, DOB: string, DateReg: string, Home Phone: string, MaritalStatus: string, EthnicCatalistName: string, Party: string, PersonalEmail: string, RaceName: string, StateFileID: string, StreetNo: string, StreetNoHalf: string, StreetPrefix: string, StreetName: string, StreetType: string, StreetSuffix: string, AptType: string, AptNo: string, City42: string, State43: string, Zip544: string, Zip445: string, General20: string, General19: string, General18: string, General17: string, General16: string, General15: string, General14: string, General12: string, General11: string, General10: string, General09: string, General08: string, General06: string, General04: string, General02: string, General00: string, General98: string, Municipal13: string, PresPrimary20: string, PresPrimary20Party: string, PresPrimary16: string, PresPrimary16Party: string, PresPrimary12: string, PresPrimary12Party: string, PresPrimary08: string, PresPrimary08Party: string, Primary20: string, Primary20Party: string, Primary19: string, Primary19Party: string, Primary18: string, Primary18Party: string, Primary17: string, Primary17Party: string, Primary16: string, Primary16Party: string, Primary15: string, Primary15Party: string, Primary14: string, Primary14Party: string, Primary12: string, Primary12Party: string, Primary11: string, Primary11Party: string, Primary10: string, Primary10Party: string, Primary09: string, Primary09Party: string, Primary08: string, Primary08Party: string, Primary06: string, Primary04: string, Primary02: string, Primary01: string, Primary00: string, Primary98: string, Special20: string, Special19: string, Special18: string, Special17: string, Special16: string, Special15: string, Special13: string, Special12: string, Special11: string, Special10: string, Special09: string, Special08: string, Special07: string, Special06: string, Special05: string, Special04: string, Special03: string, Special02: string, Special01: string, Special00: string, Special99: string, Special98: string, 2020_Biden_Support: string, MarijuanaConviction: string, Mass_Incarceration: string, PoliceAccountability: string, Absentee_Voting: string, Ballot_Application: string, VBM_Application: string, Voting_Aug_Prim: string, Voting_Nov_Gen : string, ElectionType: string, CommunicationType: string, File: string, State44: string, Zip446: string, Zip545: string, City43: string, VotedStatus: string]>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py:2213: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n",
            "  warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xxe7xyYgrbO"
      },
      "source": [
        "Bucketized = spark.read.option(\"header\",True).csv(WorkingFiles + \"Bucketized.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsK0gHkCCZtk",
        "outputId": "f2a7d74c-dd18-457c-89c0-5a395261ab61"
      },
      "source": [
        "VoterColumns = [\"Voter File VANID\", \"FirstName\", \"MiddleName\", \"LastName\", \"Address\", \"DWID\",  'Age', 'Primary19', 'MaritalStatus', 'mCity', 'Mass_Incarceration', 'StreetType', 'StreetSuffix',\n",
        "  'mAddress', 'mState', 'mZip5', 'mZip4', 'Sex', 'Suffix', 'CD', 'SD', 'HD', 'PreferredEmail', 'Preferred Phone', 'Cell Phone', 'CountyName', 'DOB', 'DateReg', 'Home Phone', \n",
        "  'EthnicCatalistName', 'Party', 'PersonalEmail', 'RaceName', 'StreetNo', 'StreetNoHalf', 'StreetPrefix', 'StreetName', 'AptType', 'AptNo', \n",
        "  '2020_Biden_Support', 'Voting_Aug_Prim', 'PoliceAccountability', 'VBM_Application', 'MarijuanaConviction',  'Absentee_Voting', 'age_bucket', 'AgeGroup',\n",
        "  'Zip545', 'State43', 'Zip412', 'StateFileID', 'Zip511', 'City42', 'Ballot_Application', 'Zip446', 'Zip445', 'Zip544'] \n",
        "\n",
        "OutreachColumns = [\"Voter File VANID\", \"ElectionType\", \"CommunicationType\", \"File\"]\n",
        "\n",
        "OtherColumns = ['Voting_Nov_Gen ',  '2020_Biden_Support']\n",
        "\n",
        "VoterColumnsToRemove = list(set(VoterColumns) - set(primary_key))\n",
        "OutreachColumnsToRemove = list(set(OutreachColumns) - set(primary_key))\n",
        "OtherColumnsToRemove = list(set(OtherColumns) - set(primary_key))\n",
        "\n",
        "\n",
        "AllColumns = set(Bucketized.columns)\n",
        "RemainingColumns = list(((set(Bucketized.columns) - set(VoterColumnsToRemove)) - set(OtherColumnsToRemove)) - set(ElectionColumns))\n",
        "\n",
        "\n",
        "VotingColumns = list(set(primary_key + RemainingColumns))\n",
        "print(RemainingColumns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Special02', 'PresPrimary08Party', 'Primary12', 'Special98', 'Special03', 'Primary12Party', 'General09', 'Primary11Party', 'Special09', 'Primary09', 'Special07', 'PresPrimary16', 'Primary98', 'Primary08Party', 'Special00', 'Primary17Party', 'Primary19Party', 'Primary06', 'General12', 'Special11', 'General15', 'Primary01', 'Special16', 'Special13', 'General19', 'AgeInt', 'Primary09Party', 'Special10', 'Primary11', 'Primary15', 'General14', 'Primary20', 'Primary10', 'Special18', 'Primary18', 'Primary18Party', 'City9', 'Special05', 'Primary16Party', 'Primary15Party', 'General18', 'PresPrimary20Party', 'General06', 'Special99', 'General02', 'Primary16', 'General20', 'PresPrimary16Party', 'Special04', 'Primary00', 'PresPrimary08', 'Special19', 'General08', 'VotedStatus', 'Special17', 'Special06', 'Special01', 'PresPrimary12Party', 'Special15', 'General17', 'General16', 'General00', 'City43', 'Primary20Party', 'State44', 'Primary14Party', 'General98', 'PresPrimary12', 'General10', 'General11', 'Municipal13', 'Primary08', 'State10', 'Special20', 'Special08', 'Primary04', 'Primary02', 'General04', 'Primary17', 'PresPrimary20', 'Primary14', 'Primary10Party', 'Special12', 'Voting_Nov_Gen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca2Ky99LqsZM"
      },
      "source": [
        "Here we are going to check all the remaining columns, and ensure that they are all voting history columns. This is a necessary step as we have received additional files throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHFmLeHUDc5Y",
        "outputId": "ea2ce227-5846-4f5e-9a0f-f9866c176cbc"
      },
      "source": [
        "dfRemaining = Bucketized.select(VotingColumns)\n",
        "dfRemaining.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+------------------+---------+---------+-------------+---------+--------------+---------+---------+---------+--------------+---------+---------+---------+---------+--------------+-------------+--------------+--------------+----------------+------------------+-------------+---------+---------+---------+---------+---------+---------+------+--------------+---------+---------+---------+---------+---------+---------+-------------+---------+--------------+---------+--------------+---------+---------+--------------+--------------+---------+---------+---------+--------------+---------+--------------+---------+---------+---------+---------+---------+------+---------+---------+---------+---------+---------+---------+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+---------+------------------+--------------+-------+-------------+---------+-----------+-------+---------+---------+---------+\n",
            "|Special02|PresPrimary08Party|Primary12|General09|PresPrimary16|Primary98|Primary17Party|Primary06|General12|Special13|Primary09Party|Special10|Primary11|Primary20|Special18|Primary18Party|        City9|Primary16Party|Primary15Party|Voter File VANID|PresPrimary16Party|PresPrimary08|Special06|Special01|Special15|General17|General16|General00|City43|Primary14Party|General98|General11|Primary08|Primary02|General04|Primary17|PresPrimary20|Primary14|Primary10Party|Special12|Voting_Nov_Gen|Special98|Special03|Primary12Party|Primary11Party|Special09|Primary09|Special07|Primary08Party|Special00|Primary19Party|Special11|General15|Primary01|Special16|General19|AgeInt|Primary15|General14|Primary10|Primary18|Special05|General18|PresPrimary20Party|General06|Special99|General02|Primary16|General20|Special04|Primary00|Special19|General08|VotedStatus|Special17|PresPrimary12Party|Primary20Party|State44|PresPrimary12|General10|Municipal13|State10|Special20|Special08|Primary04|\n",
            "+---------+------------------+---------+---------+-------------+---------+--------------+---------+---------+---------+--------------+---------+---------+---------+---------+--------------+-------------+--------------+--------------+----------------+------------------+-------------+---------+---------+---------+---------+---------+---------+------+--------------+---------+---------+---------+---------+---------+---------+-------------+---------+--------------+---------+--------------+---------+---------+--------------+--------------+---------+---------+---------+--------------+---------+--------------+---------+---------+---------+---------+---------+------+---------+---------+---------+---------+---------+---------+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+---------+------------------+--------------+-------+-------------+---------+-----------+-------+---------+---------+---------+\n",
            "|     null|              null|        P|        P|            P|     null|          null|        Y|        P|        Y|          null|     null|     null|        P|     null|          null|      Detroit|             D|          null|           82502|                 D|            P|     null|        Y|     null|     null|        P|     null|  null|          null|     null|     null|        P|        Y|        Y|        P|            P|        P|          null|     null|          null|     null|     null|          null|          null|        P|        P|     null|          null|     null|          null|     null|     null|        Y|     null|     null|    77|     null|        P|        P|        P|        Y|        P|              null|        Y|     null|        Y|        P|        A|     null|     null|     null|        P|       null|        P|              null|          null|   null|         null|        P|          Y|     MI|        P|     null|     null|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        P|     null|          null|     null|     null|     null|     null|          null|      Detroit|          null|          null|           85510|              null|         null|     null|     null|     null|     null|        P|     null|  null|          null|     null|     null|     null|     null|        Y|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    49|     null|     null|     null|     null|     null|        P|              null|     null|     null|     null|     null|        P|     null|     null|     null|     null|       null|     null|              null|          null|   null|         null|        P|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|        P|        P|            P|     null|          null|        Y|        P|        Y|          null|     null|     null|        A|     null|          null|      Detroit|             D|          null|           71010|                 D|         null|     null|        Y|     null|        P|        P|        A|  null|          null|     null|     null|     null|        Y|        Y|        P|            A|        P|          null|     null|          null|     null|     null|          null|          null|        P|        A|     null|          null|     null|          null|     null|     null|        Y|     null|     null|    67|     null|        P|        P|     null|        Y|        A|              null|        Y|     null|        Y|        P|        A|     null|     null|     null|        A|       null|        P|              null|          null|   null|         null|        P|          Y|     MI|        A|     null|        Y|\n",
            "|     null|              null|        A|        A|         null|     null|          null|        A|        A|        Y|          null|     null|     null|        A|     null|          null|      Detroit|          null|          null|           44565|              null|            A|     null|        Y|        A|     null|        A|        A|  null|          null|     null|     null|        A|        A|        A|     null|            A|        A|          null|     null|          null|     null|        A|          null|          null|        A|        A|        A|          null|     null|          null|     null|     null|        A|     null|     null|   101|     null|        A|        A|     null|        Y|        A|              null|        A|     null|        A|     null|        A|     null|     null|     null|        A|       null|     null|              null|          null|   null|            A|        A|          Y|     MI|        A|     null|        A|\n",
            "|     null|              null|        P|        P|            P|     null|          null|        Y|        A|        Y|          null|     null|     null|        P|     null|          null|      Detroit|             D|          null|          128077|                 D|            P|     null|        Y|        P|        P|        P|        Y|  null|          null|     null|        P|        P|        Y|        Y|        P|            P|        P|          null|     null|          null|     null|        Y|          null|          null|        P|        P|        Y|          null|     null|          null|        Y|     null|     null|     null|     null|    77|     null|        P|        P|        P|        Y|        P|              null|        Y|     null|        Y|        P|        A|     null|     null|     null|        P|       null|        P|              null|          null|   null|            P|        P|          Y|     MI|        A|     null|        Y|\n",
            "|     null|              null|     null|        P|         null|     null|          null|     null|     null|     null|          null|     null|     null|     null|     null|          null|      Detroit|          null|          null|           44706|              null|         null|     null|        Y|     null|     null|        A|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|        Y|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    82|     null|     null|     null|     null|     null|     null|              null|        Y|     null|        Y|     null|        A|     null|     null|     null|     null|       null|     null|              null|          null|   null|         null|     null|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|        P|        P|            P|     null|          null|        Y|        A|     null|          null|     null|     null|        A|     null|          null|      Detroit|             D|          null|           83085|                 D|            P|     null|        Y|        P|        P|        P|        Y|  null|          null|     null|     null|        P|        Y|        Y|     null|            A|        P|          null|     null|          null|     null|        Y|          null|          null|        P|        P|        Y|          null|     null|          null|     null|     null|        Y|     null|     null|    61|     null|        P|        P|        P|        Y|        P|              null|        Y|     null|        Y|        P|        A|     null|     null|     null|        A|       null|     null|              null|          null|   null|         null|        P|          Y|     MI|        A|     null|        Y|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        P|     null|          null|     null|     null|     null|     null|          null|      Detroit|          null|          null|           53057|              null|         null|     null|     null|     null|     null|        P|     null|  null|          null|     null|     null|     null|     null|     null|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    65|     null|        P|     null|        P|     null|     null|              null|     null|     null|     null|     null|     null|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|     null|          Y|     MI|     null|     null|     null|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        A|     null|          null|     null|     null|     null|     null|          null|       Warren|          null|          null|           88921|              null|         null|     null|     null|     null|     null|        P|        Y|  null|          null|     null|     null|     null|     null|     null|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    81|     null|     null|     null|     null|     null|     null|              null|     null|     null|        Y|     null|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|     null|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|        A|        A|         null|     null|          null|        A|        A|        Y|          null|     null|     null|        A|     null|          null|      Detroit|          null|          null|           57000|              null|            A|     null|        A|        A|        A|        A|        A|  null|          null|     null|        A|        A|        A|        A|        A|            A|        A|          null|     null|          null|     null|        A|          null|          null|        A|        A|        A|          null|     null|          null|        A|     null|        A|     null|     null|    84|     null|        A|        A|        A|        Y|        A|              null|        A|     null|        A|     null|        A|     null|     null|     null|        A|       null|        A|              null|          null|   null|            A|        A|          Y|     MI|     null|     null|        A|\n",
            "|     null|              null|     null|        P|         null|     null|          null|     null|     null|        Y|          null|     null|     null|     null|     null|          null|      Detroit|          null|          null|           57166|              null|         null|     null|        Y|     null|        P|        P|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|        P|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    69|     null|     null|     null|     null|        Y|        P|              null|        Y|     null|        Y|     null|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|          Y|     MI|     null|     null|     null|\n",
            "|     null|              null|     null|        P|            P|     null|          null|     null|        P|     null|          null|     null|     null|        P|     null|          null|       Warren|             D|          null|           58370|                 D|         null|     null|     null|        P|     null|        P|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|            P|        P|          null|     null|          null|     null|        Y|          null|          null|        Y|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    78|     null|        A|     null|     null|     null|        P|              null|        Y|     null|        Y|        P|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        P|     null|          null|     null|     null|        P|     null|          null|      Detroit|          null|          null|           64541|              null|            P|     null|        Y|     null|     null|        P|        Y|  null|          null|     null|     null|        P|     null|        Y|        P|            P|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|        Y|     null|     null|    46|     null|        P|     null|        P|        Y|        P|              null|        Y|     null|        Y|     null|        A|     null|     null|     null|        P|       null|        P|              null|          null|   null|         null|        P|       null|     MI|        P|     null|        Y|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        P|        Y|          null|     null|     null|        A|     null|          null|      Detroit|          null|          null|           77328|              null|         null|     null|     null|     null|     null|        A|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    66|     null|     null|     null|     null|        Y|     null|              null|        Y|     null|     null|     null|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|          Y|     MI|        A|     null|     null|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        P|     null|          null|     null|     null|     null|     null|          null|  Clinton Twp|          null|          null|           49153|              null|         null|     null|        Y|     null|     null|     null|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    54|     null|     null|     null|     null|     null|     null|              null|     null|     null|        Y|     null|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|     null|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|     null|     null|            P|     null|          null|     null|        P|     null|          null|     null|     null|     null|     null|          null|St Clr Shores|             D|          null|           58654|                 D|         null|     null|        Y|     null|     null|        A|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|         null|        P|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    73|     null|        P|     null|        P|        Y|        P|              null|        Y|     null|     null|        P|     null|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|        P|     null|         null|     null|          null|     null|        P|     null|          null|     null|     null|        A|     null|          null|      Detroit|          null|          null|           82536|              null|         null|     null|     null|     null|     null|        P|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|            A|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    76|     null|        P|     null|        A|        Y|        A|              null|        Y|     null|     null|     null|        P|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|        P|     null|            P|     null|          null|     null|        P|        Y|          null|     null|     null|        P|     null|          null|      Detroit|             D|          null|          125881|                 D|         null|     null|        Y|     null|     null|        P|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|            P|     null|          null|     null|          null|     null|     null|          null|          null|        P|        P|     null|          null|     null|          null|     null|     null|        Y|     null|     null|    68|     null|        P|     null|        P|        Y|        A|              null|        Y|     null|        Y|        P|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|          Y|     MI|        A|     null|     null|\n",
            "|     null|              null|     null|     null|         null|     null|          null|     null|        P|     null|          null|     null|     null|     null|     null|          null|     Oak Park|          null|          null|           38339|              null|         null|     null|     null|     null|     null|     null|        Y|  null|          null|     null|     null|     null|     null|        Y|     null|         null|     null|          null|     null|          null|     null|     null|          null|          null|     null|     null|     null|          null|     null|          null|     null|     null|     null|     null|     null|    79|     null|     null|     null|     null|     null|     null|              null|     null|     null|        Y|     null|     null|     null|     null|     null|     null|       null|     null|              null|          null|   null|         null|     null|       null|     MI|     null|     null|     null|\n",
            "|     null|              null|     null|        P|            P|     null|          null|     null|        P|     null|          null|     null|     null|        P|     null|          null|   Eastpointe|             D|          null|           44636|                 D|         null|     null|        Y|     null|        P|        P|        Y|  null|          null|     null|     null|     null|        Y|        Y|     null|            P|     null|          null|     null|          null|     null|     null|          null|          null|        P|        P|     null|          null|     null|          null|     null|     null|        Y|     null|     null|    60|     null|        P|     null|        P|        Y|        P|              null|        Y|     null|        A|        P|        A|     null|     null|     null|        P|       null|     null|              null|          null|   null|         null|        P|       null|     MI|        P|     null|        Y|\n",
            "+---------+------------------+---------+---------+-------------+---------+--------------+---------+---------+---------+--------------+---------+---------+---------+---------+--------------+-------------+--------------+--------------+----------------+------------------+-------------+---------+---------+---------+---------+---------+---------+------+--------------+---------+---------+---------+---------+---------+---------+-------------+---------+--------------+---------+--------------+---------+---------+--------------+--------------+---------+---------+---------+--------------+---------+--------------+---------+---------+---------+---------+---------+------+---------+---------+---------+---------+---------+---------+------------------+---------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+---------+------------------+--------------+-------+-------------+---------+-----------+-------+---------+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YFafWfGdwpj"
      },
      "source": [
        "Voter demographics dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLVJ6JLrEmHl",
        "outputId": "0ec672eb-8854-4b3d-eb9f-6f9a826357ca"
      },
      "source": [
        "dfVoters = Bucketized.select(VoterColumns)\n",
        "SaveSingleFile(dfVoters, WorkingFiles + \"dfVoters.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ3F5cF8d11U"
      },
      "source": [
        "Outreach dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeutdwPgE2gJ",
        "outputId": "91dd1ec7-7f65-4bc2-d218-7f762b8ce27a"
      },
      "source": [
        "#Pull out the file information, this indicates which outreach efforts they were included in.\n",
        "dfOutreach = Bucketized.select(ElectionColumns)\n",
        "SaveSingleFile(dfOutreach, WorkingFiles + \"dfOutreach.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GYHglEWoW7d"
      },
      "source": [
        "Plotly FIPS Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVGHDLi1d6yF"
      },
      "source": [
        "This is where we call the FIPS data so that we are able to plot it with plotly. Here we are mapping the plotly geojson data to County Name to obtain the FIPS code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QSVn9xcobmX",
        "outputId": "ecdfbed4-7920-49f2-9d18-38525b4d3cec"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import json\n",
        "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
        "    counties = json.load(response)\n",
        "\n",
        "counties[\"features\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'geometry': {'coordinates': [[[-86.496774, 32.344437],\n",
              "    [-86.717897, 32.402814],\n",
              "    [-86.814912, 32.340803],\n",
              "    [-86.890581, 32.502974],\n",
              "    [-86.917595, 32.664169],\n",
              "    [-86.71339, 32.661732],\n",
              "    [-86.714219, 32.705694],\n",
              "    [-86.413116, 32.707386],\n",
              "    [-86.411172, 32.409937],\n",
              "    [-86.496774, 32.344437]]],\n",
              "  'type': 'Polygon'},\n",
              " 'id': '01001',\n",
              " 'properties': {'CENSUSAREA': 594.436,\n",
              "  'COUNTY': '001',\n",
              "  'GEO_ID': '0500000US01001',\n",
              "  'LSAD': 'County',\n",
              "  'NAME': 'Autauga',\n",
              "  'STATE': '01'},\n",
              " 'type': 'Feature'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWlkZy_oVlD"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "\n",
        "data = {}\n",
        "rows = []\n",
        "nest1 = counties[\"features\"]\n",
        "for thing in nest1: \n",
        "  if thing['properties']['STATE'] == '26':\n",
        "    data[thing['id']] = thing['properties']['NAME']\n",
        "    rows.append(Row(FIPS= thing['id'], CountyName= thing['properties']['NAME']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPz8FEFreIPx"
      },
      "source": [
        "Now we will create a few functions to assist with melting the elections dataframe down from being columnar to row-wise. This means we will have multiple rows per Voter but for various elections over the years. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYxvXeELcog_"
      },
      "source": [
        "from pyspark.sql.functions import array, col, explode, lit, struct, isnan, when, count, sum\n",
        "from pyspark.sql import DataFrame\n",
        "from typing import Iterable \n",
        "\n",
        "primary_key = [\"Voter File VANID\"]\n",
        "columns_processed = []\n",
        "\n",
        "def melt(\n",
        "        df: DataFrame, \n",
        "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
        "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
        "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
        "\n",
        "    # Create array<struct<variable: str, value: ...>>\n",
        "    _vars_and_vals = array(*(\n",
        "        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n",
        "        for c in value_vars))\n",
        "    #print(_vars_and_vals)\n",
        "    # Add to the DataFrame and explode\n",
        "    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n",
        "\n",
        "    cols = id_vars + [\n",
        "            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
        "    return _tmp.select(*cols)\n",
        "\n",
        "def renameColumns(df, ColumnList1, ColumnList2):\n",
        "  for i in range(len(ColumnList1)):\n",
        "    df = df.withColumnRenamed(ColumnList1[i], ColumnList2[i])\n",
        "  return df\n",
        "\n",
        "def ReshapeDataframeYears(df, DropNa = False):\n",
        "  ColumnsToTransform = [\"Special##\", \"General##\", \"Primary##\", \"PresPrimary##\", \"PresPrimary##Party\", \"Primary##Party\", \"Municipal##\"]\n",
        "  #DropNA is an optional argument for if we want to drop null values later on. for one of our analyses we need null values to determine non-voters. \n",
        "  melted_dataframes = []\n",
        "  runningDataFrame = None\n",
        "\n",
        "  for name in ColumnsToTransform:\n",
        "    ColumnsForName = []\n",
        "    years = []\n",
        "    number_index = name.index(\"##\")\n",
        "    for column in df.columns:\n",
        "      if len(column) < len(name):\n",
        "        continue\n",
        "      first = column[0:number_index]\n",
        "      name_first = name[0:number_index]\n",
        "      number = column[number_index:number_index+2]\n",
        "      if len(column) - number_index+2 == 0:\n",
        "        second = \"\"\n",
        "        name_second = \"\"\n",
        "      else:\n",
        "        second = column[number_index+2:len(column)]\n",
        "        name_second = name[number_index+2:len(name)]\n",
        "\n",
        "      if first == name_first and second == name_second:\n",
        "        ColumnsForName.append(column)\n",
        "        columns_processed.append(column)\n",
        "        year = column[len(first):len(first)+2]\n",
        "\n",
        "        if int(year) < 50:\n",
        "          year = \"20\" + year\n",
        "        else:\n",
        "          year = \"19\" + year\n",
        "        years.append(year)\n",
        "\n",
        "    df_reduced = df.select(primary_key + ColumnsForName)\n",
        "    df_reduced = renameColumns(df_reduced, (primary_key + ColumnsForName), (primary_key + years))\n",
        "    df_melted = melt(df_reduced, id_vars=primary_key, value_vars=(years))\n",
        "    \n",
        "    #Restructure the columns\n",
        "    final_columns = primary_key.copy()\n",
        "    final_columns.append(\"year\")\n",
        "    final_columns.append(\"participation\")\n",
        "    #print(df_melted.show())\n",
        "    df_melted = renameColumns(df_melted, df_melted.columns, final_columns)\n",
        "    df_melted = df_melted.withColumn(\"ElectionType\", F.lit(name))\n",
        "    #Drop records where we don't have participation data. Waffle df needs NA values so making an optional argument\n",
        "    #if DropNa == 'Y':\n",
        "    #  df_melted = df_melted.na.drop()\n",
        "    #else: \n",
        "    #  df_melted\n",
        "    #print(df_melted.head())\n",
        "    melted_dataframes.append(df_melted)\n",
        "    if runningDataFrame is None:\n",
        "      runningDataFrame = df_melted\n",
        "    else:\n",
        "      runningDataFrame = runningDataFrame.union(df_melted)\n",
        "  return runningDataFrame\n",
        "\n",
        "def CombineMeltedDataframes(melted_dataframes):\n",
        "  #Now join them all back together\n",
        "  #Rather silly that this is my method, but I tried a few different versions to find the most memory efficient way to do this\n",
        "  return pd.concat(melted_dataframes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qks-7aTxrC9f"
      },
      "source": [
        "Visual check that the reformed data looks appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baWTopBVFk22",
        "outputId": "5a12684c-b864-41fc-fffb-82f2603bcd1b"
      },
      "source": [
        "melteddfs = ReshapeDataframeYears(dfRemaining, False)\n",
        "melteddfs = melteddfs.na.drop()\n",
        "melteddfs.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+-------------+------------+\n",
            "|Voter File VANID|year|participation|ElectionType|\n",
            "+----------------+----+-------------+------------+\n",
            "|           82502|2013|            Y|   Special##|\n",
            "|           82502|2001|            Y|   Special##|\n",
            "|           82502|2009|            P|   Special##|\n",
            "|           82502|2005|            Y|   Special##|\n",
            "|           82502|2017|            P|   Special##|\n",
            "|           82502|2020|            P|   Special##|\n",
            "|           71010|2013|            Y|   Special##|\n",
            "|           71010|2001|            Y|   Special##|\n",
            "|           71010|2009|            P|   Special##|\n",
            "|           71010|2005|            Y|   Special##|\n",
            "|           71010|2017|            P|   Special##|\n",
            "|           71010|2020|            A|   Special##|\n",
            "|           44565|2013|            Y|   Special##|\n",
            "|           44565|2001|            Y|   Special##|\n",
            "|           44565|2015|            A|   Special##|\n",
            "|           44565|2003|            A|   Special##|\n",
            "|           44565|2009|            A|   Special##|\n",
            "|           44565|2007|            A|   Special##|\n",
            "|           44565|2005|            Y|   Special##|\n",
            "|           44565|2020|            A|   Special##|\n",
            "+----------------+----+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za6P1t5Ue0ap"
      },
      "source": [
        "Now we will save it as dfreshapedUnique for consistency of what we created with pandas earlier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcoS-ZniwlR-",
        "outputId": "079e692c-0177-4974-e0a2-f25f01597c1d"
      },
      "source": [
        "SortingColumns = ['RaceName', 'Sex', 'AgeGroup', 'CountyName']\n",
        "SaveSingleFile(melteddfs, WorkingFiles + \"dfreshapedUnique.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhOzSSRde7JV"
      },
      "source": [
        "Now we are going to put together the ACLU Voter information which is used for the Map graph we have. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NTscdPtCAWv",
        "outputId": "c9ff918a-c90a-4030-e5bc-eb291f283e0f"
      },
      "source": [
        "#Put Togther Voter Information DataFrame\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import upper, col\n",
        "SortingColumns = ['RaceName', 'Sex', 'AgeGroup', 'CountyName']\n",
        "\n",
        "AllRegisteredVoters = spark.read.option(\"header\",True).csv(WorkingDirectory + 'AdditionalData/All Registered Voters Michigan.csv')\n",
        "AllRegisteredVoters = AllRegisteredVoters.withColumnRenamed('County', 'CountyName')\n",
        "ACLUVotersGrouped = dfVoters.groupby(SortingColumns).count()\n",
        "dfVoters = dfVoters.withColumn('CountyName', upper(dfVoters['CountyName']))\n",
        "ACLUVotersJoineWithAllVotersInfo = AllRegisteredVoters.join(ACLUVotersGrouped, ['CountyName'], 'inner')\n",
        "ACLUVotersJoineWithAllVotersInfo = ACLUVotersJoineWithAllVotersInfo.withColumn(\"fraction\", (F.col(\"count\") / F.col(\"All Registered Voters\")))\n",
        "\n",
        "print(\"All registred Voters (Number of Counties): \" + str(AllRegisteredVoters.count()))\n",
        "print(\"ACLUVotersGrouped (Number of Counties) \" + str(ACLUVotersGrouped.count()))\n",
        "print(\"dfVoters (Number of Counties Post Join) \" + str(AllRegisteredVoters.count()))\n",
        "\n",
        "SaveSingleFile(ACLUVotersJoineWithAllVotersInfo ,WorkingFiles+ \"total_voter_pop.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All registred Voters (Number of Counties): 83\n",
            "ACLUVotersGrouped (Number of Counties) 2689\n",
            "dfVoters (Number of Counties Post Join) 83\n",
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOSYhHo4uNqZ"
      },
      "source": [
        "#ACLUVotersJoineWithAllVotersInfo.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwYzjR7ifCDI"
      },
      "source": [
        "We have a lot of graphs which are taking some counts based off of a few grouping columns so we will write a function here to handle that with our primary key (Voter File VANID), Sorting Columns (Demographics), and then whatever additional columns we are grouping with. We will then save it to a file to be used on the dashboard later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8XEfcO1meDT"
      },
      "source": [
        "def SummarizeForValue(df, ColumnsWeWantValuesFor, FileName, dropna = True):\n",
        "  selected = df.select(SortingColumns + primary_key + ColumnsWeWantValuesFor)\n",
        "  selected = selected.na.drop()\n",
        "  selected = selected.groupBy(SortingColumns + ColumnsWeWantValuesFor).count()\n",
        "  SaveSingleFile(selected, WorkingFiles + FileName)\n",
        "  return selected\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pcavVeOQewz",
        "outputId": "496aaf37-cd4d-487c-b27a-309e9e6a228b"
      },
      "source": [
        "SummarizeForValue(Bucketized, ['Mass_Incarceration'], \"mass_inc.csv\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq5WpnFEUJ_6",
        "outputId": "32f8ead2-c401-49ab-a8e5-68d8f20b8a17"
      },
      "source": [
        "print(Bucketized.columns)\n",
        "SummarizeForValue(Bucketized, ['CommunicationType', 'ElectionType'], \"AggregateByCommunication.csv\");\n",
        "SummarizeForValue(Bucketized, ['PoliceAccountability'], \"policing.csv\");\n",
        "SummarizeForValue(Bucketized, ['MarijuanaConviction'], \"weed.csv\");\n",
        "SummarizeForValue(Bucketized, ['2020_Biden_Support'], \"biden_support_df.csv\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Voter File VANID', 'DWID', 'mAddress', 'mCity', 'mState', 'mZip5', 'mZip4', 'Sex', 'Address', 'City9', 'State10', 'Zip511', 'Zip412', 'LastName', 'FirstName', 'MiddleName', 'Suffix', 'CD', 'SD', 'HD', 'PreferredEmail', 'Preferred Phone', 'Age', 'Cell Phone', 'CountyName', 'DOB', 'DateReg', 'Home Phone', 'MaritalStatus', 'EthnicCatalistName', 'Party', 'PersonalEmail', 'RaceName', 'StateFileID', 'StreetNo', 'StreetNoHalf', 'StreetPrefix', 'StreetName', 'StreetType', 'StreetSuffix', 'AptType', 'AptNo', 'City42', 'State43', 'Zip544', 'Zip445', 'General20', 'General19', 'General18', 'General17', 'General16', 'General15', 'General14', 'General12', 'General11', 'General10', 'General09', 'General08', 'General06', 'General04', 'General02', 'General00', 'General98', 'Municipal13', 'PresPrimary20', 'PresPrimary20Party', 'PresPrimary16', 'PresPrimary16Party', 'PresPrimary12', 'PresPrimary12Party', 'PresPrimary08', 'PresPrimary08Party', 'Primary20', 'Primary20Party', 'Primary19', 'Primary19Party', 'Primary18', 'Primary18Party', 'Primary17', 'Primary17Party', 'Primary16', 'Primary16Party', 'Primary15', 'Primary15Party', 'Primary14', 'Primary14Party', 'Primary12', 'Primary12Party', 'Primary11', 'Primary11Party', 'Primary10', 'Primary10Party', 'Primary09', 'Primary09Party', 'Primary08', 'Primary08Party', 'Primary06', 'Primary04', 'Primary02', 'Primary01', 'Primary00', 'Primary98', 'Special20', 'Special19', 'Special18', 'Special17', 'Special16', 'Special15', 'Special13', 'Special12', 'Special11', 'Special10', 'Special09', 'Special08', 'Special07', 'Special06', 'Special05', 'Special04', 'Special03', 'Special02', 'Special01', 'Special00', 'Special99', 'Special98', '2020_Biden_Support', 'MarijuanaConviction', 'Mass_Incarceration', 'PoliceAccountability', 'Absentee_Voting', 'Ballot_Application', 'VBM_Application', 'Voting_Aug_Prim', 'Voting_Nov_Gen', 'ElectionType', 'CommunicationType', 'File', 'State44', 'Zip446', 'Zip545', 'City43', 'VotedStatus', 'AgeInt', 'age_bucket', 'AgeGroup']\n",
            "Writing File\n",
            "Moving file\n",
            "Writing File\n",
            "Moving file\n",
            "Writing File\n",
            "Moving file\n",
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MxeiaxPQkcE"
      },
      "source": [
        "Now to process the specific voting records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MntKIASyKG0T",
        "outputId": "1da6e067-e2ab-47ba-d0d6-202ea760cf28"
      },
      "source": [
        "VoterHistory = Bucketized.drop(\"ElectionType\").join(melteddfs, ['Voter File VANID'], 'inner')\n",
        "election_votes = SummarizeForValue(VoterHistory, ['ElectionType', 'year'], \"election_votes.csv\")\n",
        "election_votes.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n",
            "+---------+---+--------+----------+--------------+----+-----+\n",
            "| RaceName|Sex|AgeGroup|CountyName|  ElectionType|year|count|\n",
            "+---------+---+--------+----------+--------------+----+-----+\n",
            "|Caucasian|  M|   45-64|     Eaton|     Special##|2015|  456|\n",
            "|Caucasian|  M|   30-44|   Lenawee|     General##|2016| 3610|\n",
            "| Hispanic|  M|   45-64|    Ottawa|     General##|2012| 1978|\n",
            "|Caucasian|  M|     65+|     Wayne|     Special##|2001| 4069|\n",
            "|Caucasian|  M|     65+|   Oakland|     General##|2011|99920|\n",
            "|    Black|  F|   30-44|   Jackson|     Special##|2017|   25|\n",
            "|Caucasian|  F|   30-44|   Genesee|     Special##|2020| 5198|\n",
            "|Caucasian|  M|   30-44|    Ottawa|     Special##|2010|  226|\n",
            "|    Asian|  M|   45-64|     Wayne|     General##|2010| 3096|\n",
            "|    Black|  M|   30-44|    Macomb|     Primary##|2018| 2473|\n",
            "|Caucasian|  F|   45-64| Van Buren|     Primary##|2018|  323|\n",
            "|    Black|  M|   30-44|   Genesee|Primary##Party|2016| 2994|\n",
            "|  Unknown|  F|   30-44|   Oakland| PresPrimary##|2008|  358|\n",
            "|Caucasian|  F|   45-64|  Manistee|     Special##|2019|    1|\n",
            "|Caucasian|  M|     65+|   Jackson|     Primary##|2012|  516|\n",
            "|Caucasian|  M|     65+|   Jackson|Primary##Party|2016|  566|\n",
            "|Caucasian|  F|   45-64|  Chippewa|     General##|2008|  271|\n",
            "|Caucasian|  F|   30-44|  Houghton|     General##|2020| 1257|\n",
            "|    Asian|  F|     65+|   Oakland|     General##|2000| 6805|\n",
            "|    Asian|  F|     65+|   Oakland|     General##|2004| 8813|\n",
            "+---------+---+--------+----------+--------------+----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoOeftxG4A_X"
      },
      "source": [
        "#VoterHistory.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOsGSIlJmRnD"
      },
      "source": [
        "Waffle DF Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92vt5xzjVECp",
        "outputId": "dd40bb1d-0aad-4c9d-b261-f1c12ac144b3"
      },
      "source": [
        "Bucketized_sub = Bucketized.select('Voter File VANID','RaceName','AgeGroup', 'CountyName', 'Sex', 'General20', \n",
        "                                   'Primary20',col(\"ElectionType\").alias(\"CampaignType\"), 'CommunicationType')\n",
        "new_col_1 = when(col(\"General20\").isNull() & col(\"Primary20\").isNull(), 1).otherwise(0)\n",
        "new_col_2 = when(col(\"General20\").isNull() & col(\"Primary20\").isNotNull(), 1).otherwise(0)\n",
        "new_col_3 = when(col(\"General20\").isNotNull() & col(\"Primary20\").isNull(), 1).otherwise(0)\n",
        "new_col_4 = when(col(\"General20\").isNotNull() & col(\"Primary20\").isNotNull(), 1).otherwise(0)\n",
        "Bucketized_sub = (Bucketized_sub.withColumn(\"NoVote\", new_col_1).withColumn(\"Primary\", new_col_2).withColumn(\"General\", new_col_3).withColumn(\"VoteBoth\", new_col_4))\n",
        "agg_t = Bucketized_sub.groupBy(SortingColumns + ['CampaignType', 'CommunicationType']).agg(sum('NoVote').alias(\"NoVote\"), \n",
        "                                                                                           sum('Primary').alias(\"Primary\"), \n",
        "                                                                                           sum('General').alias(\"General\"), \n",
        "                                                                                           sum('VoteBoth').alias(\"VoteBoth\"))\n",
        "SaveSingleFile(agg_t, WorkingFiles + \"waffle_df.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVClHM8Vr0FQ"
      },
      "source": [
        "General Election Participation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbAX5TjZr6zD"
      },
      "source": [
        "def participation(FileName):\n",
        "  election_20 = melteddfs.where(melteddfs.year == 2020)\n",
        "  gen_20 = election_20.where(election_20.ElectionType == 'General##')\n",
        "  dfVoters_dem = dfVoters.select(['Voter File VANID','RaceName','AgeGroup', 'CountyName', 'Sex' ])\n",
        "  gen_20_dem = gen_20.join(dfVoters_dem,['Voter File VANID'] , 'left')\n",
        "  selected = gen_20_dem.groupBy(['RaceName','AgeGroup', 'CountyName', 'Sex','participation']).count()\n",
        "  SaveSingleFile(selected, WorkingFiles + FileName)\n",
        "  return selected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xArsiKNtsOHm",
        "outputId": "dae78d6b-6a26-4471-8d29-28fd62b9cdff"
      },
      "source": [
        "party = participation('gen20_participation.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing File\n",
            "Moving file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OktxcDaXTVfQ"
      },
      "source": [
        "Outreach Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reIhzb85Wzpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f084b9dc-382f-47af-c578-6c4f6c8d93d8"
      },
      "source": [
        "dfVoters_outreach = dfOutreach.join(dfVoters, ['Voter File VANID'], 'inner')\n",
        "print(dfVoters_outreach.show())\n",
        "dfOutreachGrouped = SummarizeForValue(dfVoters_outreach, ['CommunicationType', 'ElectionType',], \"dfoutreachSumm.csv\")\n",
        "dfOutreachGrouped.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+------------+-----------------+--------------------+---------+----------+--------+--------------------+---------+---+---------+-------------+------------+------------------+----------+------------+--------------------+------+-----+-----+---+------+---+---+---+--------------+---------------+-------------+----------+----------+----------+-------------+------------------+-----+-------------+---------+--------+------------+------------+----------+-------+-----+------------------+---------------+--------------------+---------------+-------------------+---------------+----------+--------+------+-------+------+-----------+------+------------+------------------+------+-------+--------+\n",
            "|Voter File VANID|ElectionType|CommunicationType|                File|FirstName|MiddleName|LastName|             Address|     DWID|Age|Primary19|MaritalStatus|       mCity|Mass_Incarceration|StreetType|StreetSuffix|            mAddress|mState|mZip5|mZip4|Sex|Suffix| CD| SD| HD|PreferredEmail|Preferred Phone|   Cell Phone|CountyName|       DOB|   DateReg|   Home Phone|EthnicCatalistName|Party|PersonalEmail| RaceName|StreetNo|StreetNoHalf|StreetPrefix|StreetName|AptType|AptNo|2020_Biden_Support|Voting_Aug_Prim|PoliceAccountability|VBM_Application|MarijuanaConviction|Absentee_Voting|age_bucket|AgeGroup|Zip545|State43|Zip412|StateFileID|Zip511|      City42|Ballot_Application|Zip446| Zip445|  Zip544|\n",
            "+----------------+------------+-----------------+--------------------+---------+----------+--------+--------------------+---------+---+---------+-------------+------------+------------------+----------+------------+--------------------+------+-----+-----+---+------+---+---+---+--------------+---------------+-------------+----------+----------+----------+-------------+------------------+-----+-------------+---------+--------+------------+------------+----------+-------+-----+------------------+---------------+--------------------+---------------+-------------------+---------------+----------+--------+------+-------+------+-----------+------+------------+------------------+------+-------+--------+\n",
            "|        10004786|         lpv|             text|LPVTExt820210621-...|Dionnedra|       Kay|    Reid|   3094 Birch Row Dr|166235834| 50|     null|            M|East Lansing|              null|        Dr|        null|   3094 Birch Row Dr|    MI|48823| 2250|  F|  null|008|023|069|          null|  =\"5178033914\"|=\"5178033914\"|    INGHAM|06/01/1971|10/26/2006|         null|           Uncoded|    U|         null|    Black|    3094|        null|        null| Birch Row|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  2250|  104642938| 48823|East Lansing|              null|  null|=\"2250\"|=\"48823\"|\n",
            "|        10005713|         lpv|             text|LPVText720210621-...|    Kevin|      John|   Miilu|1210 Streamwood D...|166235381| 32|     null|            S|      Dallas|              null|        Dr|           N|4400 W University...|    TX|75209| 3891|  M|  null|007|024|071|          null|  =\"2149308684\"|=\"2149308684\"|     EATON|04/22/1989|11/01/2006|=\"5179771607\"|              null|    U|         null|Caucasian|    1210|        null|        null|Streamwood|    Apt|   3B|              null|           null|                null|           null|               null|           null|       1.0|   30-44|  null|     MI|  8912|   32026276| 48917|     Lansing|              null|  null|=\"8912\"|=\"48917\"|\n",
            "|        10006023|       scomi|             text|SCOMITexts2021062...|  Frances|     Maria| Youssef|   1019 Berkshire Rd|166236538| 53|     null|            M|   Ann Arbor|              null|        Rd|        null|   1019 Berkshire Rd|    MI|48104| 2753|  F|  null|012|018|053|          null|  =\"6464360643\"|=\"6464360643\"| WASHTENAW|07/12/1967|11/01/2006|=\"7343277935\"|           Mexican|    U|         null|Caucasian|    1019|        null|        null| Berkshire|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  2753|  102150909| 48104|   Ann Arbor|              null|  null|=\"2753\"|=\"48104\"|\n",
            "|        10007540|         lpv|             call|LPVCall120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             call|LPVCall120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             call|LPVCall120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             call|LPVCall120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             mail|LPVMail220210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             mail|LPVMail220210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             mail|LPVMail220210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             mail|LPVMail220210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText120210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText520210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText520210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText520210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10007540|         lpv|             text|LPVText520210621-...|Thomasina| Christine|   Davis|   21503 Dexter Blvd|166237145| 53|     null|            S|      Warren|              null|      Blvd|        null|   21503 Dexter Blvd|    MI|48089| 3482|  F|  null|009|009|022|          null|  =\"3133546770\"|=\"3133546770\"|    MACOMB|06/01/1968|11/02/2006|         null|           Uncoded|    U|         null|    Black|   21503|        null|        null|    Dexter|   null| null|              null|           null|                null|           null|               null|           null|       2.0|   45-64|  null|     MI|  3482|  105423677| 48089|      Warren|              null|  null|=\"3482\"|=\"48089\"|\n",
            "|        10011831|         lpv|             text|LPVText720210621-...|    Louis|    Victor|   Paris|  256 N Southgate Dr|166241311| 39|     null|            M|       Gwinn|              null|        Dr|        null|  256 N Southgate Dr|    MI|49841| 9098|  M|     V|001|038|109|          null|  =\"9064584143\"|=\"9064584143\"| MARQUETTE|05/21/1982|11/03/2006|         null|              null|    U|         null|Caucasian|     256|        null|           N| Southgate|   null| null|              null|           null|                null|           null|               null|           null|       1.0|   30-44|  null|     MI|  9098|  104032226| 49841|       Gwinn|              null|  null|=\"9098\"|=\"49841\"|\n",
            "+----------------+------------+-----------------+--------------------+---------+----------+--------+--------------------+---------+---+---------+-------------+------------+------------------+----------+------------+--------------------+------+-----+-----+---+------+---+---+---+--------------+---------------+-------------+----------+----------+----------+-------------+------------------+-----+-------------+---------+--------+------------+------------+----------+-------+-----+------------------+---------------+--------------------+---------------+-------------------+---------------+----------+--------+------+-------+------+-----------+------+------------+------------------+------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n",
            "Writing File\n",
            "Moving file\n",
            "+---------+---+--------+----------+-----------------+--------------------+-----+\n",
            "| RaceName|Sex|AgeGroup|CountyName|CommunicationType|        ElectionType|count|\n",
            "+---------+---+--------+----------+-----------------+--------------------+-----+\n",
            "|Caucasian|  F|   45-64|   OAKLAND|             text|                 lpv|32894|\n",
            "|Caucasian|  F|     65+|   OAKLAND|             text|             oakland|78415|\n",
            "|    Asian|  M|   45-64|    MACOMB|             mail|                 lpv| 1724|\n",
            "|    Asian|  F|   45-64|   OAKLAND|            phone|             oakland|  834|\n",
            "|  Unknown|  M|   30-44|   OAKLAND|             text|oakland county pr...|  886|\n",
            "|Caucasian|  F|   30-44|     BARRY|             call|                 lpv| 1097|\n",
            "|Caucasian|  F|   18-29|   GENESEE|             text|                 lpv|14644|\n",
            "|    Black|  F|   30-44|   GENESEE|             text|               scomi| 2070|\n",
            "|    Asian|  F|   18-29| WASHTENAW|             text|                 lpv| 3443|\n",
            "| Hispanic|  M|   18-29|       BAY|             call|                 lpv|  111|\n",
            "|Caucasian|  M|   18-29| MARQUETTE|             text|               scomi|  181|\n",
            "|Caucasian|  F|     65+| WASHTENAW|        postcards|                 lpv|  274|\n",
            "|    Asian|  M|   18-29|    MACOMB|             mail|                 lpv| 1245|\n",
            "|Caucasian|  F|     65+|  ISABELLA|             text|                 lpv|  574|\n",
            "|    Black|  M|     65+|  KALKASKA|             call|                 lpv|    1|\n",
            "| Hispanic|  M|   45-64|   ALLEGAN|             call|                 lpv|  156|\n",
            "|    Black|  M|     65+|   OAKLAND|            phone|  oakland prosecutor| 1723|\n",
            "|Caucasian|  M|   45-64|    OTTAWA|             text|               scomi|  306|\n",
            "| Hispanic|  F|     65+| WASHTENAW|             text|               scomi|   54|\n",
            "|    Black|  M|   18-29|    MONROE|             text|                 lpv|  138|\n",
            "+---------+---+--------+----------+-----------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}